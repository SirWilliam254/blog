---
layout: post
title: Titanic Machine learning problem in PYTHON.
date: 2021-04-1 13:32:20 +0300
description: This is the hello world of machine learning. # Add post description (optional)
img: q.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Data-Science, Machine-Learning]
---
## April 15, 1912
The widely considered “unsinkable” RMS Titanic met a worthy opponent "iceberg" on her maiden voyage. Due to damages sustained she took a deep dive. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, it is of interest to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).
## Reading in the data
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file Input/Output (e.g. pd.read_csv)
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```
## Inspecting the data

```python
train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
train_data.head()
```
The data.head() line returns a few rows of the whole dataset
### Training data

| index |PassengerId | Survived | Pclass	|Name	|Sex	|Age	|SibSp	|Parch	|Ticket	|Fare	|Cabin	|Embarked|
|------|------|----------|-----------|-------|-------|-------|-------|-------|-------|-------|--------|---------|
|0	|1	|0	|3	|Braund, Mr. Owen Harris	|male	|22.0	|1	|0	|A/5 21171	|7.2500|	NaN |	S |
|1	|2	|1	|1|	Cumings, Mrs. John Bradley (Florence Briggs Th...	|female	|38.0|	1|	0|	PC 17599|	71.2833	|C85|	C |
|2	|3	|1	|3	|Heikkinen, Miss. Laina	|female	|26.0	|0	|0	|STON/O2. 3101282	|7.9250|	NaN|	S |
|3	|4	|1	|1	|Futrelle, Mrs. Jacques Heath (Lily May Peel)	|female	|35.0	|1	|0	|113803|	53.1000	|C123	|S |
|4	|5	|0	|3	|Allen, Mr. William Henry	|male	|35.0	|0	|0	|373450|	8.0500|	NaN	|S |

Looking at the output for the training data we see that there is alot of Nans in the Cabin column. The rest of the data for the small 
sample on display looks natural. We will perform more exploratory analysis on them to select features.

```python
test_data = pd.read_csv("/kaggle/input/titanic/test.csv")
test_data.head()
```
### Test data

|index |PassengerId	|Pclass|	Name|	Sex|	Age	|SibSp	|Parch|	Ticket|	Fare|	Cabin	|Embarked |
|-----|-------|----------|-----------|-------|-------|-------|-------|-------|-------|-------|--------|
|0	|892	|3	|Kelly, Mr. James	|male|	34.5	|0|	0|	330911|	7.8292|	NaN|	Q|
|1	|893	|3	|Wilkes, Mrs. James (Ellen Needs)	|female|	47.0|	1|	0	|363272	|7.0000|	NaN|	S|
|2	|894	|2	|Myles, Mr. Thomas Francis|	male|	62.0|	0|	0|	240276|	9.6875	|NaN	|Q|
|3	|895	|3	|Wirz, Mr. Albert	|male|	27.0|	0	|0	|315154|	8.6625|	NaN	|S|
|4	|896	|3	|Hirvonen, Mrs. Alexander (Helga E Lindqvist)|	female|	22.0|	1|	1	|3101298|	12.2875	|NaN	|S|

Just like for the training data, the test data has many Nans...the rest of the variables look natural.

```python
women = train_data.loc[train_data.Sex == 'female']["Survived"]
rate_women = sum(women)/len(women)

print("% of women who survived:", rate_women)
```
### % of women who survived: 0.7420382165605095
```python
men = train_data.loc[train_data.Sex == 'male']["Survived"]
rate_men = sum(men)/len(men)

print("% of men who survived:", rate_men)
```
### % of men who survived: 0.18890814558058924
## Random Forest 
This is a supervised kind of learning algorithm. It averages the result from different decision trees i.e an ensemble of 
decision trees. They combine multiple trees to predict the class of the dataset.

```python
from sklearn.ensemble import RandomForestClassifier

y = train_data["Survived"]

features = ["Pclass", "Sex", "SibSp", "Parch"]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('my_submission.csv', index=False)
```
# Model score is: 0.77511
